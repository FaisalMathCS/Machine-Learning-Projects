{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with Ridge and Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the assignment, you need the predict the price of the house (i.e., column 4 of the csv file) and the features provided to you are 'len', 'width', 'rooms' (i.e., the first 3 columns of the csv file). You can use the sklearn library to use ``LinearRegression``, ``Ridge``, and ``Lasso`` from ``sklearn.linear_model``. Moreover, if you feel the need to expand the features to polynomials (say degree 2) you can either transform the CSV file manually or use the ``PolynomialFeatures`` from ``sklearn.preprocessing``. You might realize that adding polynomial features can improve the results but you have to be careful about overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard includes\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Routines for linear regression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "# Set label size for plots\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('LandPriceTrain.csv', delimiter=',')\n",
    "features = ['len', 'width', 'rooms']\n",
    "x_train = data[:,0:3] # predictors\n",
    "y_train = data[:,3] # response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('LandPriceTest.csv', delimiter=',')\n",
    "x_test = data[:,0:3] # predictors\n",
    "y_test = data[:,3] # response variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What best can we acheive if we have no predictors and only response (House Prices) values in the training data? What will be the mean error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  84779.45\n",
      "Mean squared error:  3210718511.6474996\n",
      "Mean error:  1.4551915228366853e-12\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "mean = np.mean(y_train)\n",
    "ME = np.mean([(price - mean) for price in y_train])\n",
    "print (\"Prediction: \", mean)\n",
    "print(\"Mean squared error: \", np.var(y_train))\n",
    "print (\"Mean error: \", ME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Let's now use the features and see what we can observe  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_subset_regression(x,y,flist):\n",
    "    if len(flist) < 1:\n",
    "        print (\"Need at least one feature\")\n",
    "        return\n",
    "    for f in flist:\n",
    "        if (f < 0) or (f > 2):\n",
    "            print (\"Feature index is out of bounds\")\n",
    "            return\n",
    "    ### COMPLETE CODE BELOW by creating an instance of LinearRegression\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(x[:,flist], y)\n",
    "    return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w =  [ 3010.83212779  2914.47821951 -2420.72225879]\n",
      "b =  -77044.07528278617\n",
      "Mean squared error (train):  217514383.15439194\n",
      "Mean error (train):  12629.086434271376\n",
      "Mean squared error (test):  158706743.7864192\n",
      "Mean error (test):  9999.972138004121\n"
     ]
    }
   ],
   "source": [
    "flist = [0,1,2]\n",
    "regr = feature_subset_regression(x_train,y_train,flist)\n",
    "print (\"w = \", regr.coef_)\n",
    "print (\"b = \", regr.intercept_)\n",
    "print (\"Mean squared error (train): \", mean_squared_error(y_train, regr.predict(x_train[:,flist])))\n",
    "print (\"Mean error (train): \", mean_absolute_error(y_train, regr.predict(x_train[:,flist])))\n",
    "print (\"Mean squared error (test): \", mean_squared_error(y_test, regr.predict(x_test[:,flist])))\n",
    "print (\"Mean error (test): \", mean_absolute_error(y_test, regr.predict(x_test[:,flist])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. It seems we are underfitting as the train and test error are significantly high. Let's try to use polynomial features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try incorporating polynomial features (say of degree 2) and see how you perform on the train and the test set. You can either transform the CSV file manually or use the ``PolynomialFeatures`` from ``sklearn.preprocessing``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "#try to expand the fetaures fit the linear regression and report the results\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias = \"False\")\n",
    "x_train_poly = poly.fit_transform(x_train)\n",
    "x_test_poly = poly.transform(x_test)\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(x_train_poly, y_train)\n",
    "\n",
    "\n",
    "y_train_pred = regr.predict(x_train_poly)\n",
    "y_test_pred = regr.predict(x_test_poly)\n",
    "\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "train_me = mean_absolute_error(y_train, y_train_pred)\n",
    "test_me = mean_absolute_error(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w =  [ 0.00000000e+00  3.38880737e+02  1.79182339e+03  2.45415349e+03\n",
      " -1.76818784e+00  8.72187773e+01 -2.26660380e+01 -2.74067566e-02\n",
      " -3.10041171e+02  9.42215698e+02]\n",
      "b =  -38854.243235337024\n",
      "Mean squared error (train):  21545554.458582617\n",
      "Mean error (train):  3338.283508427424\n",
      "Mean squared error (test):  94462869.08423862\n",
      "Mean error (test):  7759.405168100534\n"
     ]
    }
   ],
   "source": [
    "### UPDATE THE CODE BELOW ###\n",
    "print (\"w = \", regr.coef_)\n",
    "print (\"b = \", regr.intercept_)\n",
    "print (\"Mean squared error (train): \", train_mse)\n",
    "print (\"Mean error (train): \", train_me)\n",
    "print (\"Mean squared error (test): \", test_mse)\n",
    "print (\"Mean error (test): \", test_me)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It seems we are overfitting as the train error is significantly lower than the test error. Let's try some regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_subset_ridge(x,y,flist, alp):\n",
    "    if len(flist) < 1:\n",
    "        print (\"Need at least one feature\")\n",
    "        return\n",
    "    for f in flist:\n",
    "        if (f < 0) or (f > 8):\n",
    "            print (\"Feature index is out of bounds\")\n",
    "            return\n",
    "    ### COMPLETE CODE BELOW by creating an instance of Ridge, be careful of the parameters\n",
    "    X = x[:, flist]\n",
    "    regr = Ridge(alp) \n",
    "    regr.fit(X, y)\n",
    "    return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w =  [ 0.00000000e+00  4.12101764e+02  1.57654547e+03  9.23423066e+03\n",
      " -2.19052832e+00  8.80722027e+01 -4.10782058e+01  1.71336832e+00\n",
      " -2.96897086e+02]\n",
      "b =  -47065.30819461866\n",
      "Mean squared error (train):  22198039.813325174\n",
      "Mean error (train):  3311.9241870382257\n",
      "Mean squared error (test):  94344288.83366063\n",
      "Mean error (test):  7840.610425059407\n"
     ]
    }
   ],
   "source": [
    "flist = [0,1,2,3,4,5,6,7,8]\n",
    "x_train = x_train_poly\n",
    "x_test = x_test_poly\n",
    "regr = feature_subset_ridge(x_train,y_train,flist, 0.05)\n",
    "print (\"w = \", regr.coef_)\n",
    "print (\"b = \", regr.intercept_)\n",
    "print (\"Mean squared error (train): \", mean_squared_error(y_train, regr.predict(x_train[:,flist])))\n",
    "print (\"Mean error (train): \", mean_absolute_error(y_train, regr.predict(x_train[:,flist])))\n",
    "print (\"Mean squared error (test): \", mean_squared_error(y_test, regr.predict(x_test[:,flist])))\n",
    "print (\"Mean error (test): \", mean_absolute_error(y_test, regr.predict(x_test[:,flist])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_subset_lasso(x,y,flist, alp):\n",
    "    if len(flist) < 1:\n",
    "        print (\"Need at least one feature\")\n",
    "        return\n",
    "    for f in flist:\n",
    "        if (f < 0) or (f > 8):\n",
    "            print (\"Feature index is out of bounds\")\n",
    "            return\n",
    "    ### COMPLETE CODE BELOW by creating an instance of Lasso, be careful of the parameters\n",
    "    regr = Lasso(alp)\n",
    "    regr.fit(x[:,flist], y)\n",
    "    return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w =  [  0.           0.           0.           0.          -2.47207915\n",
      "  94.95087354  50.45538117  10.62218684 -65.22359262]\n",
      "b =  -2800.9911741478863\n",
      "Mean squared error (train):  32376302.916847847\n",
      "Mean error (train):  4501.866664200646\n",
      "Mean squared error (test):  51356247.313329\n",
      "Mean error (test):  6399.21983148757\n"
     ]
    }
   ],
   "source": [
    "flist = [0,1,2,3,4,5,6,7,8]\n",
    "regr = feature_subset_lasso(x_train,y_train,flist, 1150)\n",
    "print (\"w = \", regr.coef_)\n",
    "print (\"b = \", regr.intercept_)\n",
    "print (\"Mean squared error (train): \", mean_squared_error(y_train, regr.predict(x_train[:,flist])))\n",
    "print (\"Mean error (train): \", mean_absolute_error(y_train, regr.predict(x_train[:,flist])))\n",
    "print (\"Mean squared error (test): \", mean_squared_error(y_test, regr.predict(x_test[:,flist])))\n",
    "print (\"Mean error (test): \", mean_absolute_error(y_test, regr.predict(x_test[:,flist])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document your observation and understanding\n",
    "(What you learn from the results, what does model parameters tell you,...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add you observations and understanding:\n",
    "1- in this problem it seems Lasso is a better regulrazition technique to use than Ridge since it offers a lower error in general both in train and test\n",
    "\n",
    "2- for some reason some of the Weights in the lasso are really close to zero\n",
    "\n",
    "3- the same goes for ridge but lasso has more Zero weights\n",
    "\n",
    "4- Feature exapnsion helped a bit but can cause overfitting too \n",
    "\n",
    "\n",
    "5- regurlazation techniqeus can help reduce the effects of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Questions (Optional-Extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implement the closed-form solution for Ridge\n",
    "2. Implement the iterative solution (Gradient Descent) for Ridge\n",
    "3. Implement the iterative solution for Lasso\n",
    "4. Use the sklearn linear_model.ElasticNet and try on the above problem.\n",
    "\n",
    "Compare your implemented solutions with the built-in solutions on the above problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
