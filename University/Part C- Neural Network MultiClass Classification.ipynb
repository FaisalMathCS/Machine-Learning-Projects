{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2csauEkooXD2"
   },
   "source": [
    " ## C. Neural Network: MultiClass Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the previous architecture to model multi-class classification task. Test your architecture on the **Vehicle Silhouettes** Data Set ('Vehicles.csv'). Save your solution as a seperate notebook file with appropriate filename.\n",
    "\n",
    "**Note:**\n",
    "1. Perform the train/validate/test split as 70/15/15.\n",
    "2. Use Random seed as '777' wherever needed.\n",
    "3. Report appropriate measures in addition to accuracy and also plot the confusion matrix.\n",
    "\n",
    "More details on the dataset can be found at: https://archive.ics.uci.edu/ml/datasets/Statlog+%28Vehicle+Silhouettes%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sqaGHZSJoXCU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faisal\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Faisal\\Anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Package imports\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the dataset and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Vehicles.csv', header=None)\n",
    "\n",
    "# Preprocessing\n",
    "# Assuming 'label_column_name' is the name of the column with the labels\n",
    "X = df.iloc[:, :-1]\n",
    "Y = df.iloc[:, -1]\n",
    "\n",
    "# Convert labels to numerical values\n",
    "# Assuming the labels are strings and there are 4 unique classes\n",
    "label_encoder = OneHotEncoder(sparse=False)\n",
    "Y_encoded = label_encoder.fit_transform(Y.values.reshape(-1, 1))\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(X_scaled, Y_encoded, test_size=0.3, random_state=777)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=777)\n",
    "\n",
    "# Reshape data for the neural network\n",
    "X_train, X_val, X_test = X_train.T, X_val.T, X_test.T\n",
    "Y_train, Y_val, Y_test = Y_train.T, Y_val.T, Y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 592)\n",
      "(4, 592)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mOsdX_bPoXCx"
   },
   "outputs": [],
   "source": [
    "def model_architecture(X, num_classes):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    num_classes -- total number of classes in the classification problem\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer (equal to the number of classes)\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### \n",
    "    n_x = X.shape[0] # size of input layer\n",
    "    n_h = 10         # size of hidden layer (can be adjusted based on model complexity)\n",
    "    n_y = 4 # size of output layer, equal to the number of classes\n",
    "    ### END CODE HERE ###\n",
    "    return (n_x, n_h, n_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XV3W6uLvoXC3"
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(2)\n",
    "\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z, axis=0):\n",
    "    \"\"\"\n",
    "    Compute the softmax of matrix Z along the specified axis.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- A numpy array of shape (n_classes, m)\n",
    "\n",
    "    Returns:\n",
    "    A -- Softmax of Z, same shape as Z\n",
    "    \"\"\"\n",
    "    # Compute the exponential of Z element-wise\n",
    "    e_Z = np.exp(Z - np.max(Z, axis=axis, keepdims=True))\n",
    "\n",
    "    # Compute softmax values along the specified axis\n",
    "    A = e_Z / np.sum(e_Z, axis=axis, keepdims=True)\n",
    "\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AkJ4TKKSoXC9"
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The softmax output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)  # tanh is often used for hidden layers\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = softmax(Z2)  # Using softmax at the output layer\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "whxXuxq-oXDD"
   },
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A2 -- The softmax output of the second activation, of shape (n_y, number of examples), where n_y is the number of classes\n",
    "    Y -- \"true\" labels vector of shape (n_y, number of examples), one-hot encoded\n",
    "\n",
    "    Returns:\n",
    "    cost -- categorical cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of examples\n",
    "\n",
    "    # Compute the categorical cross-entropy cost\n",
    "    logprobs = np.multiply(Y, np.log(A2))\n",
    "    cost = - np.sum(logprobs) / m\n",
    "    \n",
    "    cost = float(np.squeeze(cost))  # ensures cost is the dimension we expect. \n",
    "\n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KHTVtf4_oXDJ"
   },
   "outputs": [],
   "source": [
    "def backprop(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data\n",
    "    Y -- \"true\" labels (one-hot encoded)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "        \n",
    "    # Retrieve A1 and A2 from dictionary \"cache\".\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    dZ2 = A2 - Y  # Simplification due to softmax and categorical cross-entropy\n",
    "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))  # Assuming tanh activation in hidden layer\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWeErPRmoXDP"
   },
   "outputs": [],
   "source": [
    "def update(parameters, grads, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    learning_rate -- The learning rate\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    ### START CODE HERE ### \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    ### START CODE HERE ### \n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    ## END CODE HERE ###\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    ### START CODE HERE ### \n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cnSyk2auoXDU"
   },
   "outputs": [],
   "source": [
    "def NeuralNetwork(X, Y, n_h, num_iterations = 10000, learning_rate = 0.01, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset\n",
    "    Y -- labels \n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    learning_rate -- The learning rate\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to make predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = model_architecture(X, Y)[0]\n",
    "    n_y = model_architecture(X, Y)[2]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        ### START CODE HERE ### \n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost(A2, Y)\n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backprop(parameters, cache, X, Y)\n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters =  update(parameters, grads, learning_rate)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YPMG_zMloXDZ"
   },
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data \n",
    "    \n",
    "    Returns:\n",
    "    predictions -- vector of predictions of our model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "\n",
    "    # Convert probabilities to class predictions\n",
    "    predictions = np.argmax(A2, axis=0)  # axis=0 as we're considering each column as an example\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 1: Accuracy = 75.59055118110236%\n",
      "Experiment 2: Accuracy = 81.10236220472441%\n",
      "Experiment 3: Accuracy = 66.14173228346458%\n",
      "Experiment 4: Accuracy = 77.16535433070865%\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(n_h, num_iterations, learning_rate, X_train, Y_train, X_val, Y_val):\n",
    "    parameters = NeuralNetwork(X_train, Y_train, n_h, num_iterations, learning_rate, print_cost=False)\n",
    "    predictions_val = predict(parameters, X_val)\n",
    "    accuracy = accuracy_score(np.argmax(Y_val, axis=0), predictions_val)\n",
    "    return accuracy\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Experiment 1\n",
    "acc1 = run_experiment(4, 10000, 0.01, X_train, Y_train, X_val, Y_val)\n",
    "results.append(('Experiment 1', acc1))\n",
    "\n",
    "# Experiment 2\n",
    "acc2 = run_experiment(8, 15000, 0.01, X_train, Y_train, X_val, Y_val)\n",
    "results.append(('Experiment 2', acc2))\n",
    "\n",
    "# Experiment 3\n",
    "acc3 = run_experiment(4, 10000, 0.005, X_train, Y_train, X_val, Y_val)\n",
    "results.append(('Experiment 3', acc3))\n",
    "\n",
    "# Experiment 4\n",
    "acc4 = run_experiment(8, 15000, 0.005, X_train, Y_train, X_val, Y_val)\n",
    "results.append(('Experiment 4', acc4))\n",
    "\n",
    "# Print results\n",
    "for exp_result in results:\n",
    "    print(f\"{exp_result[0]}: Accuracy = {exp_result[1]*100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.386517\n",
      "Cost after iteration 100: 1.383564\n",
      "Cost after iteration 200: 1.378010\n",
      "Cost after iteration 300: 1.363870\n",
      "Cost after iteration 400: 1.335744\n",
      "Cost after iteration 500: 1.300696\n",
      "Cost after iteration 600: 1.271036\n",
      "Cost after iteration 700: 1.248848\n",
      "Cost after iteration 800: 1.230650\n",
      "Cost after iteration 900: 1.213228\n",
      "Cost after iteration 1000: 1.194381\n",
      "Cost after iteration 1100: 1.172599\n",
      "Cost after iteration 1200: 1.147039\n",
      "Cost after iteration 1300: 1.117445\n",
      "Cost after iteration 1400: 1.084018\n",
      "Cost after iteration 1500: 1.047318\n",
      "Cost after iteration 1600: 1.008240\n",
      "Cost after iteration 1700: 0.968110\n",
      "Cost after iteration 1800: 0.928597\n",
      "Cost after iteration 1900: 0.891234\n",
      "Cost after iteration 2000: 0.856975\n",
      "Cost after iteration 2100: 0.826145\n",
      "Cost after iteration 2200: 0.798644\n",
      "Cost after iteration 2300: 0.774168\n",
      "Cost after iteration 2400: 0.752353\n",
      "Cost after iteration 2500: 0.732840\n",
      "Cost after iteration 2600: 0.715309\n",
      "Cost after iteration 2700: 0.699479\n",
      "Cost after iteration 2800: 0.685114\n",
      "Cost after iteration 2900: 0.672012\n",
      "Cost after iteration 3000: 0.660002\n",
      "Cost after iteration 3100: 0.648944\n",
      "Cost after iteration 3200: 0.638716\n",
      "Cost after iteration 3300: 0.629218\n",
      "Cost after iteration 3400: 0.620367\n",
      "Cost after iteration 3500: 0.612092\n",
      "Cost after iteration 3600: 0.604334\n",
      "Cost after iteration 3700: 0.597041\n",
      "Cost after iteration 3800: 0.590169\n",
      "Cost after iteration 3900: 0.583682\n",
      "Cost after iteration 4000: 0.577544\n",
      "Cost after iteration 4100: 0.571725\n",
      "Cost after iteration 4200: 0.566198\n",
      "Cost after iteration 4300: 0.560937\n",
      "Cost after iteration 4400: 0.555922\n",
      "Cost after iteration 4500: 0.551129\n",
      "Cost after iteration 4600: 0.546542\n",
      "Cost after iteration 4700: 0.542143\n",
      "Cost after iteration 4800: 0.537915\n",
      "Cost after iteration 4900: 0.533843\n",
      "Cost after iteration 5000: 0.529916\n",
      "Cost after iteration 5100: 0.526118\n",
      "Cost after iteration 5200: 0.522440\n",
      "Cost after iteration 5300: 0.518870\n",
      "Cost after iteration 5400: 0.515397\n",
      "Cost after iteration 5500: 0.512014\n",
      "Cost after iteration 5600: 0.508710\n",
      "Cost after iteration 5700: 0.505480\n",
      "Cost after iteration 5800: 0.502314\n",
      "Cost after iteration 5900: 0.499208\n",
      "Cost after iteration 6000: 0.496156\n",
      "Cost after iteration 6100: 0.493153\n",
      "Cost after iteration 6200: 0.490194\n",
      "Cost after iteration 6300: 0.487277\n",
      "Cost after iteration 6400: 0.484399\n",
      "Cost after iteration 6500: 0.481558\n",
      "Cost after iteration 6600: 0.478752\n",
      "Cost after iteration 6700: 0.475982\n",
      "Cost after iteration 6800: 0.473246\n",
      "Cost after iteration 6900: 0.470545\n",
      "Cost after iteration 7000: 0.467879\n",
      "Cost after iteration 7100: 0.465248\n",
      "Cost after iteration 7200: 0.462652\n",
      "Cost after iteration 7300: 0.460090\n",
      "Cost after iteration 7400: 0.457562\n",
      "Cost after iteration 7500: 0.455066\n",
      "Cost after iteration 7600: 0.452597\n",
      "Cost after iteration 7700: 0.450150\n",
      "Cost after iteration 7800: 0.447720\n",
      "Cost after iteration 7900: 0.445306\n",
      "Cost after iteration 8000: 0.442923\n",
      "Cost after iteration 8100: 0.440598\n",
      "Cost after iteration 8200: 0.438351\n",
      "Cost after iteration 8300: 0.436181\n",
      "Cost after iteration 8400: 0.434081\n",
      "Cost after iteration 8500: 0.432040\n",
      "Cost after iteration 8600: 0.430052\n",
      "Cost after iteration 8700: 0.428111\n",
      "Cost after iteration 8800: 0.426214\n",
      "Cost after iteration 8900: 0.424357\n",
      "Cost after iteration 9000: 0.422540\n",
      "Cost after iteration 9100: 0.420758\n",
      "Cost after iteration 9200: 0.419012\n",
      "Cost after iteration 9300: 0.417300\n",
      "Cost after iteration 9400: 0.415620\n",
      "Cost after iteration 9500: 0.413972\n",
      "Cost after iteration 9600: 0.412355\n",
      "Cost after iteration 9700: 0.410767\n",
      "Cost after iteration 9800: 0.409207\n",
      "Cost after iteration 9900: 0.407675\n",
      "Cost after iteration 10000: 0.406170\n",
      "Cost after iteration 10100: 0.404690\n",
      "Cost after iteration 10200: 0.403236\n",
      "Cost after iteration 10300: 0.401806\n",
      "Cost after iteration 10400: 0.400400\n",
      "Cost after iteration 10500: 0.399016\n",
      "Cost after iteration 10600: 0.397655\n",
      "Cost after iteration 10700: 0.396314\n",
      "Cost after iteration 10800: 0.394994\n",
      "Cost after iteration 10900: 0.393694\n",
      "Cost after iteration 11000: 0.392413\n",
      "Cost after iteration 11100: 0.391151\n",
      "Cost after iteration 11200: 0.389906\n",
      "Cost after iteration 11300: 0.388679\n",
      "Cost after iteration 11400: 0.387468\n",
      "Cost after iteration 11500: 0.386274\n",
      "Cost after iteration 11600: 0.385095\n",
      "Cost after iteration 11700: 0.383930\n",
      "Cost after iteration 11800: 0.382780\n",
      "Cost after iteration 11900: 0.381645\n",
      "Cost after iteration 12000: 0.380522\n",
      "Cost after iteration 12100: 0.379412\n",
      "Cost after iteration 12200: 0.378315\n",
      "Cost after iteration 12300: 0.377230\n",
      "Cost after iteration 12400: 0.376156\n",
      "Cost after iteration 12500: 0.375094\n",
      "Cost after iteration 12600: 0.374042\n",
      "Cost after iteration 12700: 0.373001\n",
      "Cost after iteration 12800: 0.371970\n",
      "Cost after iteration 12900: 0.370949\n",
      "Cost after iteration 13000: 0.369937\n",
      "Cost after iteration 13100: 0.368934\n",
      "Cost after iteration 13200: 0.367940\n",
      "Cost after iteration 13300: 0.366955\n",
      "Cost after iteration 13400: 0.365978\n",
      "Cost after iteration 13500: 0.365009\n",
      "Cost after iteration 13600: 0.364048\n",
      "Cost after iteration 13700: 0.363095\n",
      "Cost after iteration 13800: 0.362149\n",
      "Cost after iteration 13900: 0.361210\n",
      "Cost after iteration 14000: 0.360279\n",
      "Cost after iteration 14100: 0.359354\n",
      "Cost after iteration 14200: 0.358436\n",
      "Cost after iteration 14300: 0.357524\n",
      "Cost after iteration 14400: 0.356619\n",
      "Cost after iteration 14500: 0.355721\n",
      "Cost after iteration 14600: 0.354828\n",
      "Cost after iteration 14700: 0.353942\n",
      "Cost after iteration 14800: 0.353061\n",
      "Cost after iteration 14900: 0.352187\n"
     ]
    }
   ],
   "source": [
    "#use the hyperparameters of the best model above ^\n",
    "parameters = NeuralNetwork(X_train, Y_train, 8, num_iterations=15000, learning_rate=0.01, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the accuracy on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BUdvhN9BoXDn",
    "outputId": "ee2a1302-2638-4fa9-efd4-eec8fefbc882"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicitino shape  (592,)\n",
      "Y train shape (4, 592)\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "predictions = predict(parameters, X_train)\n",
    "print(\"predicitino shape \",predictions.shape)\n",
    "print(\"Y train shape\", Y_train.shape)\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# report = classification_report(Y_train, predictions)\n",
    "# print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Y_train shape: (592,)\n"
     ]
    }
   ],
   "source": [
    "# Convert one-hot encoded labels to class labels\n",
    "Y_train_labels = np.argmax(Y_train, axis=0)\n",
    "\n",
    "# Now Y_train_labels should have the same shape as predictions\n",
    "print(\"Converted Y_train shape:\", Y_train_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error and other metrics: \n",
      "Accuracy: 0.839527027027027\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96       154\n",
      "           1       0.74      0.72      0.73       158\n",
      "           2       0.71      0.72      0.71       148\n",
      "           3       0.95      0.98      0.96       132\n",
      "\n",
      "    accuracy                           0.84       592\n",
      "   macro avg       0.84      0.84      0.84       592\n",
      "weighted avg       0.84      0.84      0.84       592\n",
      "\n",
      "Confusion Matrix:\n",
      " [[149   1   2   2]\n",
      " [  2 113  40   3]\n",
      " [  3  37 106   2]\n",
      " [  1   1   1 129]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "print(\"Training Error and other metrics: \")\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(Y_train_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(Y_train_labels, predictions)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Generate a confusion matrix\n",
    "conf_matrix = confusion_matrix(Y_train_labels, predictions)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error and other metrics: \n",
      "Accuracy: 0.8267716535433071\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94        33\n",
      "           1       0.61      0.54      0.57        26\n",
      "           2       0.72      0.74      0.73        31\n",
      "           3       0.97      0.97      0.97        37\n",
      "\n",
      "    accuracy                           0.83       127\n",
      "   macro avg       0.80      0.81      0.80       127\n",
      "weighted avg       0.82      0.83      0.82       127\n",
      "\n",
      "Confusion Matrix:\n",
      " [[32  0  0  1]\n",
      " [ 3 14  9  0]\n",
      " [ 0  8 23  0]\n",
      " [ 0  1  0 36]]\n"
     ]
    }
   ],
   "source": [
    "predictions_test = predict(parameters, X_test)\n",
    "Y_test_labels = np.argmax(Y_test, axis=0)\n",
    "print(\"Test Error and other metrics: \")\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(Y_test_labels, predictions_test)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(Y_test_labels, predictions_test)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Generate a confusion matrix\n",
    "conf_matrix = confusion_matrix(Y_test_labels, predictions_test)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WvoglA35oXD3"
   },
   "source": [
    "References:\n",
    "- https://www.coursera.org/learn/neural-networks-deep-learning\n",
    "- http://scs.ryerson.ca/~aharley/neural-networks/\n",
    "- http://cs231n.github.io/neural-networks-case-study/\n",
    "- https://archive.ics.uci.edu/ml/datasets.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Assignment8.ipynb",
   "provenance": []
  },
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "wRuwL",
   "launcher_item_id": "NI888"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
