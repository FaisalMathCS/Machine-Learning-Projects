{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## D. Neural Network: Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the developed architecture to model regression task. Test your architecture on the **Airfoil Self-Noise** Data Set ('Airfoil.csv'). Save your solution as a seperate notebook file with appropriate filename.\n",
    "\n",
    "**Note:**\n",
    "1. Perform the train/validate/test split as 70/15/15.\n",
    "2. Use Random seed as '777' wherever needed.\n",
    "3. Report appropriate measures like MSE, MAE.\n",
    "\n",
    "More details on the dataset can be found at: https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sqaGHZSJoXCU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faisal\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the dataset and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "X_train: (5, 1051)\n",
      "Y_train: (1051,)\n",
      "X_val: (5, 225)\n",
      "Y_val: (225,)\n",
      "X_test: (5, 226)\n",
      "Y_test: (226,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Airfoil.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "X = data.iloc[:, :-1]  # all columns except the last one\n",
    "Y = data.iloc[:, -1]   # only the last column\n",
    "X = X.values\n",
    "Y = Y.values\n",
    "\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.3, random_state=777)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=777)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = X_train.T \n",
    "X_val = X_val.T \n",
    "X_test = X_test.T\n",
    "# Shapes of the datasets\n",
    "print(\"Shapes of the datasets:\")\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"Y_train:\", Y_train.shape)\n",
    "print(\"X_val:\", X_val.shape)\n",
    "print(\"Y_val:\", Y_val.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"Y_test:\", Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mOsdX_bPoXCx"
   },
   "outputs": [],
   "source": [
    "def model_architecture(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### \n",
    "    n_x = X.shape[0] # size of input layer\n",
    "    n_h = 10\n",
    "    n_y = 1 \n",
    "    ### END CODE HERE ###\n",
    "    return (n_x, n_h, n_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XV3W6uLvoXC3"
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer (number of classes for multiclass)\n",
    "\n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(2)\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AkJ4TKKSoXC9"
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The output of the second activation (linear)\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Implement Forward Propagation to calculate A2\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = Z2  # Linear activation function\n",
    "    \n",
    "    assert(A2.shape == (parameters[\"W2\"].shape[0], X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "whxXuxq-oXDD"
   },
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A2 -- The output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (number of examples,)\n",
    "       \n",
    "    Returns:\n",
    "    cost -- mean squared error cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.size  # number of examples\n",
    "\n",
    "    # Reshape Y to match the shape of A2\n",
    "    Y = Y.reshape(A2.shape)\n",
    "    \n",
    "    # Compute the mean squared error cost\n",
    "    cost = (1/m) * np.sum((A2 - Y)**2)\n",
    "    \n",
    "    cost = float(np.squeeze(cost))\n",
    "\n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KHTVtf4_oXDJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def backprop(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data\n",
    "    Y -- \"true\" labels\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    ### START CODE HERE ### \n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    ### START CODE HERE ### \n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    ### START CODE HERE ### \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWeErPRmoXDP"
   },
   "outputs": [],
   "source": [
    "def update(parameters, grads, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    learning_rate -- The learning rate\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    ### START CODE HERE ### \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    ### START CODE HERE ### \n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    ## END CODE HERE ###\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    ### START CODE HERE ### \n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cnSyk2auoXDU"
   },
   "outputs": [],
   "source": [
    "def NeuralNetwork(X, Y, n_h, num_iterations = 10000, learning_rate = 0.01, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset\n",
    "    Y -- labels \n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    learning_rate -- The learning rate\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to make predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = model_architecture(X, Y)[0]\n",
    "    n_y = model_architecture(X, Y)[2]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        ### START CODE HERE ### \n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost(A2, Y)\n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backprop(parameters, cache, X, Y)\n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters =  update(parameters, grads, learning_rate)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YPMG_zMloXDZ"
   },
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data \n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute output using forward propagation\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = A2.flatten()  # Flatten to ensure it is one-dimensional\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resulst on the Validation set!\n",
      "Experiment 1: RMSE = 3.962694025920766\n",
      "Experiment 2: RMSE = 2.929347178469321\n",
      "Experiment 3: RMSE = 4.696265341772368\n",
      "Experiment 4: RMSE = 3.024782259392391\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def run_experiment(n_h, num_iterations, learning_rate, X_train, Y_train, X_val, Y_val):\n",
    "    parameters = NeuralNetwork(X_train, Y_train, n_h, num_iterations, learning_rate, print_cost=False)\n",
    "    predictions_val = predict(parameters, X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(Y_val, predictions_val))\n",
    "    return rmse\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Run experiments\n",
    "exp1_rmse = run_experiment(4, 10000, 0.01, X_train, Y_train, X_val, Y_val)\n",
    "results.append(('Experiment 1', exp1_rmse))\n",
    "\n",
    "exp2_rmse = run_experiment(8, 15000, 0.01, X_train, Y_train, X_val, Y_val)\n",
    "results.append(('Experiment 2', exp2_rmse))\n",
    "\n",
    "exp3_rmse = run_experiment(4, 10000, 0.005, X_train, Y_train, X_val, Y_val)\n",
    "results.append(('Experiment 3', exp3_rmse))\n",
    "\n",
    "exp4_rmse = run_experiment(8, 15000, 0.005, X_train, Y_train, X_val, Y_val)\n",
    "results.append(('Experiment 4', exp4_rmse))\n",
    "\n",
    "# Print results\n",
    "print(\"resulst on the Validation set!\")\n",
    "for exp_result in results:\n",
    "    print(f\"{exp_result[0]}: RMSE = {exp_result[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "op5NM0gXoXDi",
    "outputId": "5459e1b0-e49c-4c62-f499-6f0564c0b49e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 15637.592061\n",
      "Cost after iteration 100: 28.018027\n",
      "Cost after iteration 200: 19.195073\n",
      "Cost after iteration 300: 18.851588\n",
      "Cost after iteration 400: 18.721332\n",
      "Cost after iteration 500: 18.650092\n",
      "Cost after iteration 600: 18.597418\n",
      "Cost after iteration 700: 18.560685\n",
      "Cost after iteration 800: 18.535111\n",
      "Cost after iteration 900: 18.516154\n",
      "Cost after iteration 1000: 18.501854\n",
      "Cost after iteration 1100: 18.490667\n",
      "Cost after iteration 1200: 18.481374\n",
      "Cost after iteration 1300: 18.473152\n",
      "Cost after iteration 1400: 18.465431\n",
      "Cost after iteration 1500: 18.457735\n",
      "Cost after iteration 1600: 18.449572\n",
      "Cost after iteration 1700: 18.440353\n",
      "Cost after iteration 1800: 18.429275\n",
      "Cost after iteration 1900: 18.415134\n",
      "Cost after iteration 2000: 18.395922\n",
      "Cost after iteration 2100: 18.367871\n",
      "Cost after iteration 2200: 18.323430\n",
      "Cost after iteration 2300: 18.250487\n",
      "Cost after iteration 2400: 18.143069\n",
      "Cost after iteration 2500: 18.010522\n",
      "Cost after iteration 2600: 17.825806\n",
      "Cost after iteration 2700: 17.318323\n",
      "Cost after iteration 2800: 16.050150\n",
      "Cost after iteration 2900: 15.621330\n",
      "Cost after iteration 3000: 15.350071\n",
      "Cost after iteration 3100: 15.149977\n",
      "Cost after iteration 3200: 14.963653\n",
      "Cost after iteration 3300: 14.717523\n",
      "Cost after iteration 3400: 14.232806\n",
      "Cost after iteration 3500: 13.960339\n",
      "Cost after iteration 3600: 13.828336\n",
      "Cost after iteration 3700: 13.724257\n",
      "Cost after iteration 3800: 13.635851\n",
      "Cost after iteration 3900: 13.557195\n",
      "Cost after iteration 4000: 13.484963\n",
      "Cost after iteration 4100: 13.417124\n",
      "Cost after iteration 4200: 13.352334\n",
      "Cost after iteration 4300: 13.289626\n",
      "Cost after iteration 4400: 13.228231\n",
      "Cost after iteration 4500: 13.167470\n",
      "Cost after iteration 4600: 13.106663\n",
      "Cost after iteration 4700: 13.045056\n",
      "Cost after iteration 4800: 12.981732\n",
      "Cost after iteration 4900: 12.915510\n",
      "Cost after iteration 5000: 12.844818\n",
      "Cost after iteration 5100: 12.767577\n",
      "Cost after iteration 5200: 12.681325\n",
      "Cost after iteration 5300: 12.583948\n",
      "Cost after iteration 5400: 12.474925\n",
      "Cost after iteration 5500: 12.355884\n",
      "Cost after iteration 5600: 13.251516\n",
      "Cost after iteration 5700: 13.211679\n",
      "Cost after iteration 5800: 13.234256\n",
      "Cost after iteration 5900: 13.262928\n",
      "Cost after iteration 6000: 13.291794\n",
      "Cost after iteration 6100: 13.319446\n",
      "Cost after iteration 6200: 13.345608\n",
      "Cost after iteration 6300: 13.370380\n",
      "Cost after iteration 6400: 13.394000\n",
      "Cost after iteration 6500: 13.416761\n",
      "Cost after iteration 6600: 13.438992\n",
      "Cost after iteration 6700: 13.461041\n",
      "Cost after iteration 6800: 13.483266\n",
      "Cost after iteration 6900: 13.506010\n",
      "Cost after iteration 7000: 13.529515\n",
      "Cost after iteration 7100: 13.553701\n",
      "Cost after iteration 7200: 13.577504\n",
      "Cost after iteration 7300: 13.597185\n",
      "Cost after iteration 7400: 13.603219\n",
      "Cost after iteration 7500: 13.580633\n",
      "Cost after iteration 7600: 13.523844\n",
      "Cost after iteration 7700: 13.445782\n",
      "Cost after iteration 7800: 13.359701\n",
      "Cost after iteration 7900: 13.270684\n",
      "Cost after iteration 8000: 13.179850\n",
      "Cost after iteration 8100: 13.086835\n",
      "Cost after iteration 8200: 12.989623\n",
      "Cost after iteration 8300: 12.883534\n",
      "Cost after iteration 8400: 12.760509\n",
      "Cost after iteration 8500: 12.610132\n",
      "Cost after iteration 8600: 12.422669\n",
      "Cost after iteration 8700: 12.193166\n",
      "Cost after iteration 8800: 11.923390\n",
      "Cost after iteration 8900: 11.448618\n",
      "Cost after iteration 9000: 10.684050\n",
      "Cost after iteration 9100: 9.925016\n",
      "Cost after iteration 9200: 9.318209\n",
      "Cost after iteration 9300: 8.884521\n",
      "Cost after iteration 9400: 8.586409\n",
      "Cost after iteration 9500: 8.378244\n",
      "Cost after iteration 9600: 8.227949\n",
      "Cost after iteration 9700: 8.116419\n",
      "Cost after iteration 9800: 8.030808\n",
      "Cost after iteration 9900: 7.961063\n",
      "Cost after iteration 10000: 7.901494\n",
      "Cost after iteration 10100: 7.863762\n",
      "Cost after iteration 10200: 7.847200\n",
      "Cost after iteration 10300: 7.784321\n",
      "Cost after iteration 10400: 7.716339\n",
      "Cost after iteration 10500: 7.652713\n",
      "Cost after iteration 10600: 7.596399\n",
      "Cost after iteration 10700: 7.546144\n",
      "Cost after iteration 10800: 7.500216\n",
      "Cost after iteration 10900: 7.457351\n",
      "Cost after iteration 11000: 7.417068\n",
      "Cost after iteration 11100: 7.379976\n",
      "Cost after iteration 11200: 7.347786\n",
      "Cost after iteration 11300: 7.320800\n",
      "Cost after iteration 11400: 7.296266\n",
      "Cost after iteration 11500: 7.274403\n",
      "Cost after iteration 11600: 7.255702\n",
      "Cost after iteration 11700: 7.239572\n",
      "Cost after iteration 11800: 7.225620\n",
      "Cost after iteration 11900: 7.213470\n",
      "Cost after iteration 12000: 7.202849\n",
      "Cost after iteration 12100: 7.193595\n",
      "Cost after iteration 12200: 7.185648\n",
      "Cost after iteration 12300: 7.179017\n",
      "Cost after iteration 12400: 7.173745\n",
      "Cost after iteration 12500: 7.169865\n",
      "Cost after iteration 12600: 7.167364\n",
      "Cost after iteration 12700: 7.166156\n",
      "Cost after iteration 12800: 7.166070\n",
      "Cost after iteration 12900: 7.166857\n",
      "Cost after iteration 13000: 7.168212\n",
      "Cost after iteration 13100: 7.169805\n",
      "Cost after iteration 13200: 7.171315\n",
      "Cost after iteration 13300: 7.172469\n",
      "Cost after iteration 13400: 7.173064\n",
      "Cost after iteration 13500: 7.172976\n",
      "Cost after iteration 13600: 7.172158\n",
      "Cost after iteration 13700: 7.170628\n",
      "Cost after iteration 13800: 7.168449\n",
      "Cost after iteration 13900: 7.165709\n",
      "Cost after iteration 14000: 7.162507\n",
      "Cost after iteration 14100: 7.158944\n",
      "Cost after iteration 14200: 7.155110\n",
      "Cost after iteration 14300: 7.151085\n",
      "Cost after iteration 14400: 7.146937\n",
      "Cost after iteration 14500: 7.142720\n",
      "Cost after iteration 14600: 7.138477\n",
      "Cost after iteration 14700: 7.134240\n",
      "Cost after iteration 14800: 7.130033\n",
      "Cost after iteration 14900: 7.125874\n"
     ]
    }
   ],
   "source": [
    "#Use the best hyperparameter from above \n",
    "parameters = NeuralNetwork(X_train, Y_train, 8, num_iterations=15000, learning_rate=0.01, print_cost=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the accuracy on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BUdvhN9BoXDn",
    "outputId": "ee2a1302-2638-4fa9-efd4-eec8fefbc882",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1051,)\n",
      "(5, 1051)\n",
      "Training measures\n",
      "MSE:  7.121771702980565\n",
      "MAE:  2.0876002136089764\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "predictions = predict(parameters, X_train)\n",
    "print(predictions.shape)\n",
    "print(X_train.shape)\n",
    "mse = np.mean((predictions - Y_train) ** 2)\n",
    "mae = np.mean(np.abs(predictions - Y_train))\n",
    "print(\"Training measures\")\n",
    "print(\"MSE: \", mse)\n",
    "print(\"MAE: \", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test measures\n",
      "MSE:  8.585186282112387\n",
      "MAE:  2.3313471402948793\n"
     ]
    }
   ],
   "source": [
    "predictions_test = predict(parameters, X_test)\n",
    "mseTest = np.mean((predictions_test - Y_test) ** 2)\n",
    "maeTest = np.mean(np.abs(predictions_test - Y_test))\n",
    "print(\"Test measures\")\n",
    "print(\"MSE: \", mseTest)\n",
    "print(\"MAE: \", maeTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WvoglA35oXD3"
   },
   "source": [
    "References:\n",
    "- https://www.coursera.org/learn/neural-networks-deep-learning\n",
    "- http://scs.ryerson.ca/~aharley/neural-networks/\n",
    "- http://cs231n.github.io/neural-networks-case-study/\n",
    "- https://archive.ics.uci.edu/ml/datasets.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Assignment8.ipynb",
   "provenance": []
  },
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "wRuwL",
   "launcher_item_id": "NI888"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
