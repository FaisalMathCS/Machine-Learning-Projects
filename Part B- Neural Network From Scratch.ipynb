{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Implementation-From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jZ7Bd9ZsoXCR"
   },
   "source": [
    "## A. Neural Network: Binary Classification \n",
    "\n",
    "In this exercise, we will learn to build a fully-connected neural network with standard architecture, i.e., only one hidden layer. We will use the `Breast cancer wisconsin (diagnostic) dataset` available in `https://scikit-learn.org/stable/datasets/index.html`. It has a total of 569 sample with two classes (Malignant and Benign). Each sample has 30 real-valued features. \n",
    "\n",
    "**This exercise will help you understand and implement:**\n",
    "- A neural network for binary classification consisting of one hidden layer with non-linear activation. You will implement ideas like forward propogation, computing the loss and the cost, bagward propogation, and parameter(weights) update. Using the trained network, you will learn to predict a class given the features of a sample.\n",
    "\n",
    "**Note:** There are multiple conventions for coding neural networks. We will follow the conventions suggested by Andrew Ng: https://www.coursera.org/learn/neural-networks-deep-learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sqaGHZSJoXCU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faisal\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the dataset and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(X, y, test_size =0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(train_data)\n",
    "train_data = scaler.transform(train_data)\n",
    "test_data = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = train_data.T\n",
    "trainy = train_labels.reshape(-1,1).T\n",
    "\n",
    "testx = test_data.T\n",
    "testy =test_labels.reshape(-1,1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 455), (1, 455), (30, 114), (1, 114))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx.shape, trainy.shape, testx.shape, testy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=trainx\n",
    "Y=trainy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "DFAQt6HYoXCh",
    "outputId": "dc61fcf2-38fb-405c-e8d2-8d25250737b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of training samples: 455\n",
      "Number of features per sample: 30\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "shape_X = X.shape\n",
    "shape_Y = Y.shape\n",
    "m = X.shape[1]  # training set size\n",
    "### END CODE HERE ###\n",
    "\n",
    "print ('No. of training samples: ' + str(m))\n",
    "print ('Number of features per sample: ' + str(shape_X[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K2AFzm-hoXCv"
   },
   "source": [
    "## 2 - Neural Network model\n",
    "\n",
    "We will train a Neural Network with a single hidden layer.\n",
    "\n",
    "**Mathematically**:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}\\tag{1}$$ \n",
    "$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$\n",
    "$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\\tag{3}$$\n",
    "$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$\n",
    "$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}$$\n",
    "\n",
    "Given the predictions on all the examples, you can also compute the cost $J$ as follows: \n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$\n",
    "\n",
    "**Important**: Building the NN will involve the following:\n",
    "    1. Specify the network structure in terms of the number of input units, number of neurons in the hidden units, ...\n",
    "    2. Initialize the parameters of the model\n",
    "    3. Loop a number of iterations:\n",
    "        - Forward propagation\n",
    "        - Compute loss and the overall cost\n",
    "        - Backward propagation\n",
    "        - Update the parameters (gradient descent)\n",
    "\n",
    "In order to make the code modular, we can implement each of the step as a function and them combine them together to build the overall model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dC3UrB-AoXCw"
   },
   "source": [
    "### 2.1 - Specify the network structure \n",
    "    - n_x: input layer size\n",
    "    - n_h: #neurons in  hidden layer (hard code a value, say 10) \n",
    "    - n_y: the size of the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mOsdX_bPoXCx"
   },
   "outputs": [],
   "source": [
    "def model_architecture(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### \n",
    "    n_x = X.shape[0] # size of input layer\n",
    "    n_h = 10\n",
    "    n_y = Y.shape[0] # size of output layer\n",
    "    ### END CODE HERE ###\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w4-FAtl-oXC3"
   },
   "source": [
    "### 2.2 - Initialize the parameters of the model\n",
    "\n",
    "- Initialize the weights matrices with random values. \n",
    "    - Use: `np.random.randn(a,b) * 0.01` to randomly initialize a matrix of shape (a,b).\n",
    "- Initialize the bias vectors as zeros. \n",
    "    - Use: `np.zeros((a,b))` to initialize a matrix of shape (a,b) with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XV3W6uLvoXC3"
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer (number of classes for multiclass)\n",
    "\n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(2)\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    ### Update THE CODE HERE ###\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AkJ4TKKSoXC9"
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    ### START CODE HERE ### \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    ### START CODE HERE ### \n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "21VnrjU3oXDC"
   },
   "source": [
    "Now that you have computed $A^{[2]}$ (in the Python variable \"`A2`\"), which contains $a^{[2](i)}$ for every example, you can compute the cost function as follows:\n",
    "\n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{7}$$\n",
    "\n",
    "- Implement the cross-entropy loss:\n",
    "$- \\sum\\limits_{i=0}^{m}  y^{(i)}\\log(a^{[2](i)})$:\n",
    "```python\n",
    "logprobs = np.multiply(np.log(A2),Y)\n",
    "cost = - np.sum(logprobs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "whxXuxq-oXDD"
   },
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "       \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (7)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), 1 - Y)\n",
    "    cost = - np.sum(logprobs) / m\n",
    "    \n",
    "    cost = float(np.squeeze(cost))\n",
    "\n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i5puT2VVoXDH"
   },
   "source": [
    "Using the cache computed during forward propagation, we can now implement backward propagation.\n",
    "\n",
    "Backpropagation is usually the hardest (most mathematical) part. You can use the following six equations as vectorized implementation:\n",
    "\n",
    "$$dZ^{[2]} = A^{[2]} - Y \\tag{8}$$ \n",
    "$$dW^{[2]} = \\frac{1}{m} dZ^{[2]}A^{[1]{T}} \\tag{9}$$ \n",
    "$$db^{[2]} = \\frac{1}{m} np.sum(dZ^{[2]}, axis = 1, keepdims = True)\\tag{10}$$ \n",
    "$$dZ^{[1]} = W^{[2]T}dZ^{[2]}*g^{[1]'}(Z^{[1]}) \\tag{11}$$ \n",
    "$$dW^{[1]} = \\frac{1}{m} dZ^{[1]}X^{{T}} \\tag{12}$$ \n",
    "$$db^{[1]} = \\frac{1}{m} np.sum(dZ^{[1]}, axis = 1, keepdims = True)\\tag{13}$$ \n",
    "\n",
    "- $*$ denotes elementwise multiplication.\n",
    "- Notations followed:\n",
    "    - dW1 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_1 }$\n",
    "    - db1 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_1 }$\n",
    "    - dW2 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_2 }$\n",
    "    - db2 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_2 }$\n",
    "   \n",
    "- Tips:\n",
    "    - To compute dZ1 you'll need to compute $g^{[1]'}(Z^{[1]})$. Since $g^{[1]}(.)$ is the tanh activation function, if $a = g^{[1]}(z)$ then $g^{[1]'}(z) = 1-a^2$. So you can compute \n",
    "    $g^{[1]'}(Z^{[1]})$ using `(1 - np.power(A1, 2))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KHTVtf4_oXDJ"
   },
   "outputs": [],
   "source": [
    "def backprop(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data\n",
    "    Y -- \"true\" labels\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PsDhe51IoXDO"
   },
   "source": [
    "Implement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).\n",
    "\n",
    "**Gradient descent rule**: $ \\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$ where $\\alpha$ is the learning rate and $\\theta$ represents a parameter.\n",
    "\n",
    "**Illustration**: The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). Images courtesy of Adam Harley.\n",
    "\n",
    "<img src=\"images\\sgd.gif\" style=\"width:400;height:400;\"> <img src=\"images\\sgd_bad.gif\" style=\"width:400;height:400;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWeErPRmoXDP"
   },
   "outputs": [],
   "source": [
    "def update(parameters, grads, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    learning_rate -- The learning rate\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    ### START CODE HERE ### \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    ### START CODE HERE ### \n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    ## END CODE HERE ###\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    ### START CODE HERE ### \n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CSWq5OE4oXDT"
   },
   "source": [
    "### 2.4 - Integrate parts 2.1, 2.2 and 2.3 in NeuralNetwork() ####\n",
    "\n",
    "Build your neural network model in `NeuralNetwork()`.\n",
    "\n",
    "**Instructions**: The neural network model has to use the previous functions in the right order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cnSyk2auoXDU"
   },
   "outputs": [],
   "source": [
    "def NeuralNetwork(X, Y, n_h, num_iterations = 10000, learning_rate = 0.01, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset\n",
    "    Y -- labels \n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    learning_rate -- The learning rate\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to make predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = model_architecture(X, Y)[0]\n",
    "    n_y = model_architecture(X, Y)[2]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        ### START CODE HERE ### \n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost(A2, Y)\n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backprop(parameters, cache, X, Y)\n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters =  update(parameters, grads, learning_rate)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Ud31U_ZoXDY"
   },
   "source": [
    "### 2.5 Predictions\n",
    "\n",
    "Use your model to predict by building predict().\n",
    "\n",
    "**Reminder**: $ predictions = \\begin{cases}\n",
    "      1 & \\text{if}\\ activation > 0.5 \\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}$  \n",
    "    \n",
    "As an example, if you would like to set the entries of a matrix X to 0 and 1 based on a threshold you would do: ```X_new = (X > threshold)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YPMG_zMloXDZ"
   },
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data \n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    ### START CODE HERE ### \n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = (A2 > 0.5).astype(int)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OC3OHe-9oXDd"
   },
   "source": [
    "## 3. Model Execution\n",
    "It is time to run the model and see how it performs on the dataset. Run the following code to test your model with a single hidden layer of $n_h$ hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "op5NM0gXoXDi",
    "outputId": "5459e1b0-e49c-4c62-f499-6f0564c0b49e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693498\n",
      "Cost after iteration 100: 0.677406\n",
      "Cost after iteration 200: 0.657364\n",
      "Cost after iteration 300: 0.589216\n",
      "Cost after iteration 400: 0.468793\n",
      "Cost after iteration 500: 0.357196\n",
      "Cost after iteration 600: 0.275707\n",
      "Cost after iteration 700: 0.220609\n",
      "Cost after iteration 800: 0.183567\n",
      "Cost after iteration 900: 0.157976\n",
      "Cost after iteration 1000: 0.139656\n",
      "Cost after iteration 1100: 0.126086\n",
      "Cost after iteration 1200: 0.115729\n",
      "Cost after iteration 1300: 0.107618\n",
      "Cost after iteration 1400: 0.101124\n",
      "Cost after iteration 1500: 0.095823\n",
      "Cost after iteration 1600: 0.091425\n",
      "Cost after iteration 1700: 0.087722\n",
      "Cost after iteration 1800: 0.084565\n",
      "Cost after iteration 1900: 0.081841\n",
      "Cost after iteration 2000: 0.079469\n",
      "Cost after iteration 2100: 0.077385\n",
      "Cost after iteration 2200: 0.075540\n",
      "Cost after iteration 2300: 0.073894\n",
      "Cost after iteration 2400: 0.072417\n",
      "Cost after iteration 2500: 0.071084\n",
      "Cost after iteration 2600: 0.069875\n",
      "Cost after iteration 2700: 0.068773\n",
      "Cost after iteration 2800: 0.067764\n",
      "Cost after iteration 2900: 0.066837\n",
      "Cost after iteration 3000: 0.065981\n",
      "Cost after iteration 3100: 0.065189\n",
      "Cost after iteration 3200: 0.064454\n",
      "Cost after iteration 3300: 0.063769\n",
      "Cost after iteration 3400: 0.063129\n",
      "Cost after iteration 3500: 0.062529\n",
      "Cost after iteration 3600: 0.061966\n",
      "Cost after iteration 3700: 0.061436\n",
      "Cost after iteration 3800: 0.060936\n",
      "Cost after iteration 3900: 0.060463\n",
      "Cost after iteration 4000: 0.060015\n",
      "Cost after iteration 4100: 0.059589\n",
      "Cost after iteration 4200: 0.059185\n",
      "Cost after iteration 4300: 0.058800\n",
      "Cost after iteration 4400: 0.058432\n",
      "Cost after iteration 4500: 0.058081\n",
      "Cost after iteration 4600: 0.057745\n",
      "Cost after iteration 4700: 0.057422\n",
      "Cost after iteration 4800: 0.057113\n",
      "Cost after iteration 4900: 0.056816\n",
      "Cost after iteration 5000: 0.056529\n",
      "Cost after iteration 5100: 0.056254\n",
      "Cost after iteration 5200: 0.055988\n",
      "Cost after iteration 5300: 0.055731\n",
      "Cost after iteration 5400: 0.055483\n",
      "Cost after iteration 5500: 0.055243\n",
      "Cost after iteration 5600: 0.055010\n",
      "Cost after iteration 5700: 0.054785\n",
      "Cost after iteration 5800: 0.054566\n",
      "Cost after iteration 5900: 0.054354\n",
      "Cost after iteration 6000: 0.054148\n",
      "Cost after iteration 6100: 0.053947\n",
      "Cost after iteration 6200: 0.053752\n",
      "Cost after iteration 6300: 0.053562\n",
      "Cost after iteration 6400: 0.053377\n",
      "Cost after iteration 6500: 0.053196\n",
      "Cost after iteration 6600: 0.053020\n",
      "Cost after iteration 6700: 0.052847\n",
      "Cost after iteration 6800: 0.052679\n",
      "Cost after iteration 6900: 0.052514\n",
      "Cost after iteration 7000: 0.052353\n",
      "Cost after iteration 7100: 0.052195\n",
      "Cost after iteration 7200: 0.052041\n",
      "Cost after iteration 7300: 0.051889\n",
      "Cost after iteration 7400: 0.051740\n",
      "Cost after iteration 7500: 0.051594\n",
      "Cost after iteration 7600: 0.051450\n",
      "Cost after iteration 7700: 0.051308\n",
      "Cost after iteration 7800: 0.051169\n",
      "Cost after iteration 7900: 0.051032\n",
      "Cost after iteration 8000: 0.050896\n",
      "Cost after iteration 8100: 0.050762\n",
      "Cost after iteration 8200: 0.050630\n",
      "Cost after iteration 8300: 0.050499\n",
      "Cost after iteration 8400: 0.050369\n",
      "Cost after iteration 8500: 0.050241\n",
      "Cost after iteration 8600: 0.050113\n",
      "Cost after iteration 8700: 0.049987\n",
      "Cost after iteration 8800: 0.049861\n",
      "Cost after iteration 8900: 0.049736\n",
      "Cost after iteration 9000: 0.049612\n",
      "Cost after iteration 9100: 0.049488\n",
      "Cost after iteration 9200: 0.049365\n",
      "Cost after iteration 9300: 0.049242\n",
      "Cost after iteration 9400: 0.049119\n",
      "Cost after iteration 9500: 0.048996\n",
      "Cost after iteration 9600: 0.048874\n",
      "Cost after iteration 9700: 0.048751\n",
      "Cost after iteration 9800: 0.048628\n",
      "Cost after iteration 9900: 0.048505\n"
     ]
    }
   ],
   "source": [
    "# Build a model with a n_h-dimensional hidden layer\n",
    "parameters = NeuralNetwork(X, Y, 4, num_iterations=10000, learning_rate=0.01, print_cost=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the accuracy on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BUdvhN9BoXDn",
    "outputId": "ee2a1302-2638-4fa9-efd4-eec8fefbc882"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.989010989010989\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "predictions = predict(parameters, X)\n",
    "print(accuracy_score(Y.T, predictions.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "predictions_test = predict(parameters, testx)\n",
    "print(accuracy_score(testy.T, predictions_test.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hrT2xrXeoXDz"
   },
   "source": [
    "## Run the model multiple times with different hyperparameters\n",
    "**Interpretation**:\n",
    "- The larger models (with more hidden units) are able to fit the training set better, until eventually the models overfit the data. \n",
    "- Too slow learning rate will take time to converge, i.e., you will need more iterations whereas too large may diverge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "66OLHmwvoXD0"
   },
   "source": [
    "**Optional questions**:\n",
    "\n",
    "Some optional questions that you can explore if you wish: \n",
    "- What happens when you change the tanh activation for a sigmoid activation or a ReLU activation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WvoglA35oXD3"
   },
   "source": [
    "References:\n",
    "- https://www.coursera.org/learn/neural-networks-deep-learning\n",
    "- http://scs.ryerson.ca/~aharley/neural-networks/\n",
    "- http://cs231n.github.io/neural-networks-case-study/\n",
    "- https://archive.ics.uci.edu/ml/datasets.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Assignment8.ipynb",
   "provenance": []
  },
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "wRuwL",
   "launcher_item_id": "NI888"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
